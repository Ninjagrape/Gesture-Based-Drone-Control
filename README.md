# Gesture-Based Drone Control

A real-time computer vision system for controlling drones using hand gestures, combining monocular depth estimation, hand pose tracking, and adaptive filtering for intuitive 3D navigation.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![OpenCV](https://img.shields.io/badge/opencv-4.x-green.svg)
![MediaPipe](https://img.shields.io/badge/mediapipe-latest-orange.svg)

## Overview

This project implements a contactless drone control interface using computer vision and machine learning. By tracking hand position and orientation in 3D space, users can control drone movement through natural gestures without physical controllers.

### Key Features

- **Custom two stage landmarker model**: A custom first stage ROI detector using traditional segmentation models. Lighterweight.
- **3D Hand Tracking**: Monocular depth estimation from single camera using hand geometry
- **Gesture Recognition**: KNN-based classifier for static and dynamic gesture recognition
- **Orientation Tracking**: Full 6-DOF hand orientation (roll, pitch, yaw) for rotation control
- **Adaptive Filtering**: One Euro Filter and velocity-based low-pass filters for smooth, jitter-free tracking
- **Real-time Visualization**: 3D drone simulator with live position and orientation feedback
- **Multi-modal Control**: Separate modes for translation, rotation, and speed control

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Camera Input      â”‚
â”‚   (640Ã—480, 30fps)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     MediaPipe Hand Landmarker       â”‚
|        or custom two stage          |
â”‚   (21 keypoints per hand, 2 hands)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gesture Recognition â”‚   â”‚  3D Position Tracker â”‚
â”‚  (KNN Classifier)    â”œâ”€â”€â–ºâ”‚  (Depth Estimation)  â”‚
â”‚  â€¢ Static gestures   â”‚   â”‚  â€¢ Multi-bone method â”‚
â”‚  â€¢ Dynamic gestures  â”‚   â”‚  â€¢ One Euro Filter   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚       Hand Orientation Tracker               â”‚    â”‚
â”‚  (Roll, Pitch, Yaw from Rotation Matrix)     â”‚    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
           â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Control State Machine                â”‚
â”‚  â€¢ Idle â†’ Translating â†’ Idle                 â”‚
â”‚  â€¢ Idle â†’ Rotating â†’ Idle                    â”‚
â”‚  â€¢ Idle â†’ Speed Control â†’ Idle               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           3D Drone Simulator                 â”‚
â”‚  â€¢ 6-DOF motion simulation                   â”‚
â”‚  â€¢ Real-time visualization                   â”‚
â”‚  â€¢ Trail rendering                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Installation

### Prerequisites

- Python 3.8 or higher
- Webcam or external camera
- Operating System: Linux, macOS, or Windows

### Dependencies

Install required packages:

```bash
pip install opencv-python mediapipe numpy pandas scikit-learn
```

### MediaPipe Model

Download the MediaPipe hand landmark model:

```bash
wget -O hand_landmarker.task https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task
```

Place `hand_landmarker.task` in the project root directory.

## Usage

### Quick Start

Run the main control system:

```bash
python knn_hand_tracking.py
```

### Gesture Commands

The system operates in different control modes:

#### Entry Gestures (Static)
- **ACCELERATE** (Closed fist âœŠ): Enter acceleration mode
- **ROTATE** (devil horns ðŸ¤™): Enter rotation mode
- **TRANSLATION** (Two fingers pointed in any comfortable directionðŸ‘†): Enter motion mode
- **GO** (pinch ðŸ¤): Release drone from safemode

#### Exit Gestures (Static)
- **STOP** (open palm, all 5 fingers extended âœ‹ ): Exit current mode and return to idle
- **HALT** (Fingers curved into a C): Exit current mode and return to idle

#### Translation Commands (3D Position)
- **Move hand left/right**: Lateral translation (>3cm threshold)
- **Move hand up/down**: Vertical translation (>3cm threshold)
- **Move hand forward/back**: Depth translation (>4cm threshold)

#### Rotation Commands (Hand Orientation)
- **Roll wrist left/right**: Roll control (>10Â° threshold)
- **Rotate arm**: Yaw control (>15Â° threshold)
- **Flop wrist up/down**: Pitch control (>10Â° threshold)

#### Speed Control Commands
- **Move hand forward**: Accelerate
- **Move hand backward**: Decelerate

### Keyboard Controls

- **Q**: Quit application
- **R**: Record gesture samples (for training)
- **F**: Reset position filter
- **S**: Set reference position (for debugging)
- **T**: Toggle between Mediapipe integrated two stage and custom two stage model
- **1-5**: Select gesture label for recording

### Drone Simulator Controls

- **Arrow Keys**: Rotate view
- **+/-**: Zoom in/out
- **R**: Reset drone position
- **Q**: Close simulator

## Technical Details

### Two-Stage Segmentation Model

The system uses a hybrid detection approach combining classical computer vision with MediaPipe:

**Stage 1 - Palm Region Detection**:
- **Skin Segmentation**: Dual HSV + YCrCb color space filtering with adaptive thresholding
- **Face Exclusion**: Haar Cascade classifier removes false positives from facial regions
- **Contour Analysis**: Detects palm via vertical projection drop-off at wrist boundary
- **Rotated Bounding Box**: Fits minimal area rectangle to palm contour, normalized to upright orientation

**Stage 2 - MediaPipe Landmark Refinement**:
- Extracts ROI from Stage 1 (256Ã—256px upright crop)
- Runs MediaPipe Hands on isolated palm region
- Maps landmarks back to original frame via inverse affine transformation
- Fallback to full-frame detection if palm segmentation fails

**Tracking**: IoU-based multi-hand tracker with One Euro filtering on bounding boxes (position: Î²=0.015, size: Î²=0.001)

### Monocular Depth Estimation

The system estimates 3D hand position from a single camera using geometric constraints:

**Method**: Multi-bone depth estimation
- Uses 7 hand bone segments (wrist-to-MCP, proximal phalanges)
- Computes depth via similar triangles: `Z = (f Ã— L_real) / L_pixel`
- Weighted averaging with outlier removal (MAD-based)
- Focal length calibration: `f â‰ˆ frame_width` pixels

**Accuracy**: Â±2-3cm at 30-100cm distance

### Gesture Recognition

**Algorithm**: K-Nearest Neighbors (KNN) with feature engineering

**Features** (126 dimensions):
- Normalized 21-landmark positions (63D)
- Pairwise distances between landmarks (21Ã—20/2 = 210 pairs â†’ reduced)
- Finger extension state (5D)
- Palm orientation normal vector (3D)

**Classification**:
- Static gestures: Spatial features only
- Dynamic gestures: Spatial + velocity features (3D motion vector)

**Training**: Interactive recording with real-time feedback

### Low-Pass Filtering

Two complementary filtering approaches:

#### One Euro Filter
- Adaptive cutoff frequency based on signal velocity
- Parameters: `min_cutoff=1.0Hz, beta=0.007, d_cutoff=1.0Hz`
- Used for: Position tracking, orientation smoothing

#### Velocity-Based Low-Pass Filter
- Smoothing adapts to hand speed
- Fast motion â†’ more responsive (Î±â‰ˆ1.0)
- Slow motion â†’ more stable (Î±â‰ˆ0.05)
- Used for: High-frequency jitter reduction

### Orientation Tracking

**Method**: Rotation matrix decomposition from hand geometry

1. Construct hand coordinate frame:
   - X-axis: Thumb â†’ Pinky
   - Z-axis: Palm normal (cross product)
   - Y-axis: Wrist â†’ Fingers (orthogonalized)

2. Extract Euler angles via XYZ decomposition:
   - Roll: Wrist rotation around finger axis
   - Pitch: Wrist flexion/extension
   - Yaw: Arm rotation in horizontal plane

3. Single-axis control:
   - Detects dominant axis (largest angular change)
   - Prevents simultaneous multi-axis commands
   - Deadzone: 10Â° (roll/pitch), 15Â° (yaw)


## Project Structure

```
gesture-drone-control/
â”œâ”€â”€ knn_hand_tracking.py           # Main control system
â”œâ”€â”€ monocular_depth_tracking.py    # 3D position estimation
â”œâ”€â”€ orientation_tracking.py        # Hand orientation tracking
â”œâ”€â”€ two_stage_detection.py         # Custom two stage model
â”œâ”€â”€ lp_filt.py                     # Low-pass filter implementations
â”œâ”€â”€ drone_simulator.py             # 3D visualization
â”œâ”€â”€ hand_landmarker.task           # MediaPipe model (download separately)
â”œâ”€â”€ gesture_static.pkl             # Trained KNN model (generated)
â”œâ”€â”€ gesture_dynamic.pkl            # Trained KNN model (generated)
â””â”€â”€ README.md                      # This file
```

## Configuration

### Depth Estimation Parameters

```python
# In monocular_depth_tracking.py
REAL_HAND_LENGTH = 19.0  # cm (adult average)
TRANSLATE_THRESHOLD = 3.0  # cm
YAW_THRESHOLD = 5.0  # cm
ACCEL_THRESHOLD = 4.0  # cm
```

### Orientation Thresholds

```python
# In orientation_tracking.py
ROLL_THRESHOLDS = {'small': 10Â°, 'medium': 20Â°, 'large': 35Â°}
YAW_THRESHOLDS = {'small': 15Â°, 'medium': 30Â°, 'large': 50Â°}
PITCH_THRESHOLDS = {'small': 10Â°, 'medium': 20Â°, 'large': 35Â°}
DEADZONE = 10.0  # degrees
```

### Filter Parameters

```python
# One Euro Filter
min_cutoff = 1.0   # Hz
beta = 0.007       # Sensitivity to velocity
d_cutoff = 1.0     # Hz (derivative smoothing)

# Velocity-Based Filter
base_alpha = 0.3
velocity_threshold = 5.0   # cm/s
max_velocity = 50.0        # cm/s
```

## Troubleshooting

### Hand Not Detected
- Ensure good lighting conditions
- Keep hand 30-100cm from camera
- Face palm toward camera for better detection
- Avoid cluttered backgrounds

### Jittery Tracking
- Adjust filter parameters (decrease `min_cutoff` for more smoothing)
- Ensure stable camera mounting
- Check for motion blur (reduce exposure time)

### Incorrect Gestures
- Retrain KNN model with more samples (press 'R' to record)
- Increase confidence threshold (adjust `MIN_STATIC_CONF`)
- Ensure clear finger extension/flexion

### Depth Estimation Issues
- Calibrate focal length for your camera
- Check lighting (depth estimation requires visible hand features)
- Adjust `REAL_HAND_LENGTH` for hand size

## Future Improvements

### Planned Features
- [ ] Deep learning-based gesture recognition
- [ ] Multi-hand coordination gestures
- [ ] Real drone hardware integration

### Research Directions
- [ ] Hand-object interaction detection
- [ ] Adaptive thresholding based on user behavior
- [ ] Temporal gesture sequences (gesture phrases)
- [ ] Transfer learning from pre-trained models
- [ ] Real-time performance optimization (GPU acceleration)

## ðŸ“š References

### Academic Papers
1. Casiez, G., Roussel, N. and Vogel, D. (2012). "1â‚¬ Filter: A Simple Speed-based Low-pass Filter for Noisy Input in Interactive Systems"


### Libraries and Tools
- [MediaPipe](https://mediapipe.dev/) - Google's ML framework for hand tracking
- [OpenCV](https://opencv.org/) - Computer vision library
- [scikit-learn](https://scikit-learn.org/) - Machine learning toolkit

## Authors

Morris Lee
The University of Sydney

## License

GNU GENERAL PUBLIC LICENSE v3

## Acknowledgments

- MediaPipe team for hand landmark detection
- Dr. Mitch Bryson for invaluable tutelege and advice
- Nikolai Goncherov for invaluable advice


---

**Note**: This is a research/educational project. For production drone control systems, additional safety features, redundancy, and regulatory compliance would be required.